\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{xcolor}

\title{DNS of Stratified Turbuluence with Rotation and Stochastic Forcing}
\author{Dante Buhl}
\date{October 2023}

\begin{document}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bmp}[1]{\begin{minipage}{#1\textwidth}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\K}{\bs{\mathrm{K}}}
\newcommand{\m}{\bs{\mu}_*}
\newcommand{\s}{\bs{\Sigma}_*}

\maketitle

\section{Previous Work}


\section{Current Work}


\section{Stochastic Forcing with Gaussian Processes}
\begin{comment}
 \left[\begin{array}{c c c c c}
    0 & \cdots & 0& \cdots& 0 \\
    \vdots & 0 & 0 & \cdots & 0 \\
    0 & 0 & \lambda_{n_c} & 0 & \vdots \\
    \vdots & \vdots & 0 & \ddots & 0 \\
    0 & 0 & \cdots & 0 & \lambda_{n}
    \end{array}\right]    
\end{comment}

My current job is to design a stochastic forcing structure using the Gaussian random process. Gaussian Processes are a way of generating a regression from current data, fitting a line almost if you will. We are using gaussian processes to use the current data to inform a new point going forward in the code. 

The concept of the Gaussian Process is not a novel idea. Its purpose is to generate new points which fit onto an informed window of uncertainty around a given set of initial data. Ultimately, the process samples a gaussian distribution whose mean and covariance matrices are created through the use of precise linear algebra and a kernel chosen to optimize on the desired properties of the gaussian regression. 

The purpose of the Gaussian Process in the context of this work is to create a statistically stationary stochastic forcing in which to perturb and drive eddies in a stable manner as done in (Waite 2004) **SOURCE**. In our Spectral Code, the Gaussian Forcing was enforced on low horizontal wavenumbers as to affect the mean background flow, without directly interacting with the turbulence structures.
\begin{align*}
    \bs{G}(\bs{k}, t) &= \left<G_x(\bs{k}, t), G_y(\bs{k}, t)\right> \\
    \bs{k_h} \cdot  \bs{G}(\bs{k}, t) &= 0
\end{align*}

Notice the restriction of the wavenumber vector being perpendicular to the horizontal Gaussian Process. This forces a restriction on the $x$ and $y$ component of our gaussian process. To ensure that the forcing be perpendicular to the wavenumber vector, we actually only need to generate a singular gaussian process and adjust its amlitude for the other component.

\begin{align*}
    \bs{k_h} \cdot  \bs{G}(\bs{k}, t) &= k_xG_x + k_yG_y = 0 \\
    G_x &= -\frac{k_y}{k_x}G_y
\end{align*}

\bmp{.47}
    \begin{align*}
        G_x = \frac{k_y}{|\bs{k_h}|}G(\bs{k}, t)
    \end{align*}
\emp
\bmp{.47}
    \begin{align*}
        G_y = \frac{-k_x}{|\bs{k_h}|}G(\bs{k}, t)
    \end{align*}
\emp

Attention must now be brought to the generation of $G(\bs{k}, t)$, which is critical to the deployment of the stochastic forcing. 

\subsection{Numerical Algorithm and Finite Precision}
The procedure in which a Gaussian Process is generated is usually not a very complex Linear Algebra structure. Given a training set, $\bs{x}$ and $\bs{y}$, both of dimension ($1 \times n$), and a test set, $\bs{x}_*$, of dimension ($1 \times n_*$), the gaussian process regression, $\bs{y}_*$ is given below.

\begin{minipage}{.45\textwidth}
\begin{align*}
    \bs{y}_* &\sim \mathcal{N}(\bs{\mu}_*, \bs{\Sigma}_*) \\
    \bs{\mu}_* &= \bs{K}_*^T \times \K^{-1} \times \bs{y} \\
    \bs{\Sigma}_* &= \K_{**} - \K_*^T \times \K^{-1} \times \K_*
\end{align*}
\end{minipage}
\begin{minipage} {.45\textwidth}
\begin{align*}
    \K &:= \bs{\mathcal{K}}(\bs{x}, \bs{x}) \\
    \K_* &:= \bs{\mathcal{K}}(\bs{x}, \bs{x}_*) \\
    \K_{**} &:= \bs{\mathcal{K}}(\bs{x}_*, \bs{x}_*)
\end{align*}
\end{minipage}


It should be noted that the covariance matrices, $\K$, are positive definite and generated through the use of the Exponential Squared Kernel, $\bs{\mathcal{K}}$. The Kernel is defined below where, $\bs{a}$ and $\bs{b}$, are vectors with all real values (i.e. $\alpha_i, \beta_j \in \R$), and the kernel function, $f$, depends on the Gaussian Scale Parameter, $\sigma$. 

\begin{minipage}{.35\textwidth}
\begin{align*}
    f(x_1, x_2) &= \exp\left(\frac{-(x_1 - x_2)^2}{2\sigma^2}\right)\\
    \bs{a} &= [\alpha_1, \alpha_2, \cdots, \alpha_n]\\
    \bs{b} &= [\beta_1, \beta_2, \cdots, \beta_m] 
\end{align*}
\end{minipage}
\begin{minipage}{.6\textwidth}
\begin{align*}
    \bs{\mathcal{K}}(\bs{a}, \bs{b})  &= \left[
    \begin{array}{c c c c}
        f(\bs{a}[1], \bs{b}[1]) &f(\bs{a}[1], \bs{b}[2]) &\cdots &f(\bs{a}[1], \bs{b}[m]) \\
        \\
        f(\bs{a}[2], \bs{b}[1]) &f(\bs{a}[2], \bs{b}[2]) &\cdots &f(\bs{a}[2], \bs{b}[m]) \\
        \vdots &\vdots &\ddots &\vdots \\
        f(\bs{a}[n], \bs{b}[1]) &f(\bs{a}[n], \bs{b}[2]) &\cdots &f(\bs{a}[n], \bs{b}[m])
    \end{array}\right]
\end{align*}

\end{minipage}

This model for the Gaussian Process works well when the matrices are well conditioned and generally, our training set of data is not very large ($n < 10^2$). However, since we will be running this code with very small timesteps ($\sim 5\cdot10^{-4}$) up to 1000 or 10,000 time units, if we don't restrict the training set, we will soon have a very ill-conditioned matrix, $\K$, and our inverse matrix, $\K^{-1}$, will be inaccurate due to finite precision errors. 
The remedy to this is to modify our equations and algorithm slightly. We introduce the concept of the psuedo inverse, so that we can discard eigenvalues and eigenvectors of our matrix, $\K$, which would be vulnerable to finite precision errors. Thus an eigendecomposition is needed in order to eliminate eigenvalues/vectors. \textcolor{red}{The matrix, $\K$, is specifically an ($n \times n$)  square, symmetric matrix by definition (this will affect the eigendecomposition routine used in the code)}. The psuedo inverse, $\K_p^{-1}$, is defined using a matrix, $\bs{Q}$, whose columns are the eigenvectors of $\K$, $\bs{\Lambda_i}$, and a diagonal matrix, $\bs{D}_p$, whose diagonal entries are the corresponding eigenvalues of $\K$, $\lambda_i$, in ascending order. We also use a relative threshold, $\tau$, such that any eigenvector less than, $\tau \cdot \lambda_n$, is discarded. The first eigenvalue above this relative tolerance is defined to be, $\lambda_{n_c}$. 

\bmp{.95}
\begin{align*}
    \K_p^{-1} &:= \bs{Q}_p \bs{D}_p^{-1} \bs{Q}_p^{T} \\
    \bs{D}_p &:= \text{diag}(\lambda_{n_c}, \cdots, \lambda_{n}) \\  
    \bs{Q}_p &:= \left[\bs{\Lambda_{n_c}}, \cdots, \bs{\Lambda_{n}}\right]   
   \end{align*}
\emp

The aforementioned prodecure to generate $\bs{y}_*$ is modified to use the pseudo inverse and its components. 

\bmp{.95}
\begin{align*}
    \bs{\mu}_*  &=  \bs{K}_*^T\K^{-1}\bs{y}  
                = (\bs{Q}_p^T\K_*)^T\bs{D}_p^{-1}(\bs{Q}_p^T\bs{x}) \\
    \bs{\Sigma}_* &= \K_{**} - \K_*^T\K^{-1}\K_* 
                = \K_{**} - (\bs{Q}_p^T\K_*)^T\bs{D}_p^{-1}(\bs{Q}_p^T\K_*) 
\end{align*}
\emp

The specific value of the tolerance used is open to interpretation and experiementation. \textcolor{red}{While the ratio of the largest and smallest eigenvalue of a matrix is related to the condition number of a matrix}. This is the assignment for $\m$ and $\s$ as used in the code. 

Our last consideration is to simplify sampling from a multivariate normal (AKA gaussian) distribution. Since our code will be written in fortran, which lacks a built in function to sample a standard normal distribution, we must build our own sampler. It is easier to sample a multivariate standard normal distribution and use the $\m, \s$ matrices to transform it to the distribution we want. Different methods of accomplishing this vary on the factorization method of $\s$. Remember for the univariate case, we have that if $X$ is distributed normally with a mean, $\mu$, and standard deviation, $\sigma$, we have, 
\[X = \mu + \sigma Z, \text{  } Z \sim \mathcal{N}(0, 1).\]
Remember that the matrix, $\s$, is representative of the covariance in our gaussian distribution. Since standard deviation is the square root of variance/covariance, the last task at hand is to choose a factorization technique which yields the ``square-root'' matrix of our original $\s$. Note this is distinct from taking the square root of the elements of $\s$. Popular methods involve the Cholesky Decomposition which is very pointed at finding the, ``square root''  matrix. In this paper, we will use an eigendecompisition to accomplish this. Normally an eigendecomposition factors a given matrix into two matrices, $\bs{Q}$ and $\bs{D}$ such that, 
\[ 
    \bs{A} = \bs{Q}\bs{D}\bs{Q}^T
\]
We then have our matrix, $\bs{M}$, which represents the square root matrix defined below. 
\begin{align*}
    \bs{M} &= \bs{Q}\bs{D}^{\frac{1}{2}} \\
    \bs{f}^* &= \m + \bs{M}\N(\bs{0}_{n^*}, \bs{\mathrm{I}}_{n^* \times n^*})
\end{align*}

This is the exact formulation used in the code for generating a Gaussian Random Process. Specifications as to how a training set, gaussian time scale, $\Delta t$, and time series generation is described in the Code Design and Algorithm Structure section. 

\subsection{Parameter Space and Training/Test Set Design}

While the algorithm for the generation of the test set from the training set is resolved, there are lingering questions in our choice of parameters ($\sigma$, $\tau$, $\Delta t$), and design of the training and test sets $(\bs{x}, \bs{x}^*)$. Ultimately, our paramters must be chosen to ensure the smoothness of the gaussian process, and also to optimize on the runtime of the GP generation. This is primarily a note of concern for $\sigma$ and $\bs{x}$. The $\tau$ parameter is the tolerance for eigenvalue cutoff and doesn't have an affect which noticeably affect the resolution or smoothness of the Gaussian Process while in a range of magnitudes between \textcolor{red}{(INSERT RANGE HERE)}. Experimentation with the test set, often depends on the structure of the training set, which as we will see can be chosen to have a sparse window structure, or a dense window structure. A small analysis will ultimately show that many of these parameters can be connected by a series of nondimensional numbers which relate different time deltas as they relate to our choice and design of paramters. 

First discussion must be brought to the windowed structure of the training set. In the numerical algorithm for the Gaussian Process generation, we see that the most taxing numerical process the Eigendecomposition process in order to compute the inverse matrix, $\K^{-1}$. Because of this, the size of our training set, has a very clear impact on the number of computations and runtime of the Gaussian Process Generation. In an effort to reduce this runtime, a windowed approach is investigated. That is, we only generate new points in the Gaussian Process, with a training set that discards points in the time series after a certain amount of time. The number of points we keep at a given time, is thus assigned to the value, $n$. This way, when we are computing a Gaussian Regression to This introduces another parameter, $n_{\text{skip}}$, which is the number of timesteps which are computed before the window is updated. Our average $\Delta t$ is 0.0005 seconds, which implies that our Gaussian Process must produce a force value on a time discretization with a point every 0.0005 seconds. 


\section{Code Design and Algorithm Structure}



\end{document}
